{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **线性回归简洁实现**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过使用深度学习框架来简洁的实现**线性回归**模型生成数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **生成数据集**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **导入所需包**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import numpy as np  \n",
    "from torch.utils import data  \n",
    "from d2l import torch as d2l  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **构造真实 $w$，$b$、及其他数据**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_w = torch.tensor([2,-3.4])         # 给定真实w，b\n",
    "true_b = 4.2  \n",
    "features, labels = d2l.synthetic_data(true_w, true_b, 1000)         # 通过synthetic_data人工数据合成函数生成features和labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **读取数据**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **调用框架中现有的API来读取数据**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[-0.4656,  0.0443],\n",
       "         [-0.4651,  0.2159],\n",
       "         [ 1.6327, -0.5312],\n",
       "         [ 0.2651,  0.9108],\n",
       "         [ 1.3119, -0.1822],\n",
       "         [ 0.2633, -0.7431],\n",
       "         [ 1.2022,  0.8866],\n",
       "         [ 0.0839, -0.3997],\n",
       "         [-1.8440, -0.2656],\n",
       "         [-0.6579,  0.7857]]),\n",
       " tensor([[3.1194],\n",
       "         [2.5300],\n",
       "         [9.2873],\n",
       "         [1.6358],\n",
       "         [7.4366],\n",
       "         [7.2506],\n",
       "         [3.5953],\n",
       "         [5.7096],\n",
       "         [1.4162],\n",
       "         [0.2307]])]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_array(data_arrays, batch_size, is_train=True):\n",
    "    \"\"\"构造一个PyTorch数据迭代器\"\"\"\n",
    "    dataset = data.TensorDataset(*data_arrays)\n",
    "    return data.DataLoader(dataset, batch_size, shuffle=is_train)       # 使用DataLoader函数每次随机从(dataset中挑选出batch_size个样本，并随机打乱顺序)\n",
    "\n",
    "batch_size = 10         \n",
    "data_iter = load_array((features, labels), batch_size)          # 构造iter\n",
    "\n",
    "next(iter(data_iter))       # 把data_iter转成python的iter，通过next函数得到x，y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **定义模型**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **使用框架的预定义好的层**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'nn' 是神经网络的缩写\n",
    "from torch import nn        # Neural Networks的缩写等价于全连接层\n",
    "\n",
    "net = nn.Sequential(nn.Linear(2, 1))   # 指定输入的维度是2，输出的维度是1；可以直接使用Linear层(单层神经网络)，把这个层放到一个有序容器中(Sequential)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **初始化模型参数**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].weight.data.normal_(0, 0.01)  # network可以通过0访问到Linear，通过.weight访问到w，通过.data访问真实data，使用正态分布替换掉data中的值\n",
    "net[0].bias.data.fill_(0)   #bias指偏差，将数据设为0 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **计算均方误差使用的是MSELoss类，也称平方范数**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.MSELoss()         # MSELoss可以直接调用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **实例化SGD(随机梯度下降)实例**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = torch.optim.SGD(net.parameters(), lr=0.03)       # SGD要求需要传入至少两个参数：①所有net.parameters包括w和b  ②学习率lr\n",
    "# optim是指optimize优化，sgd是优化的一种方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **训练过程**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.000154\n",
      "epoch 2, loss 0.000097\n",
      "epoch 3, loss 0.000098\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 3          # 迭代3个周期\n",
    "for epoch in range(num_epochs):         # 对每一次迭代扫一遍数据\n",
    "    for X, y in data_iter:          # 从data_iter中把X，y拿出来\n",
    "        l = loss(net(X), y)         # 放进net里面，此处net()中自带模型参数，所以不需要把参数写进去，得到预测值y和真实y做loss\n",
    "        trainer.zero_grad()         # trainer优化器把梯度清零\n",
    "        l.backward()            # 计算backward，这里pytorch以及计算过sum就不需要再写\n",
    "        trainer.step()          # 调用step函数更新模型\n",
    "    l = loss(net(features), labels)         # 扫完一遍数据之后，将所有的features放到network中，和labels做loss\n",
    "    print(f'epoch {epoch + 1}, loss {l:f}')     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **对比结果**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面我们比较生成数据集的真实参数和通过有限数据训练获得的模型参数。 要访问参数，我们首先从net访问所需的层，然后读取该层的权重和偏置。 正如在从零开始实现中一样，我们估计得到的参数与生成数据的真实参数非常接近。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w的估计误差为: tensor([ 0.0004, -0.0012])\n",
      "b的估计误差为: tensor([0.0004])\n"
     ]
    }
   ],
   "source": [
    "w = net[0].weight.data  \n",
    "print('w的估计误差为:', true_w - w.reshape(true_w.shape))\n",
    "b = net[0].bias.data  \n",
    "print('b的估计误差为:',true_b - b)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3b09108e514517bd5e4a6e228723cfb42d49fc72c35aeaac957cf3be0497138a"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('py38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
